<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>functorch.vmap &mdash; functorch preview documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
      <link rel="stylesheet" href="../_static/css/pytorch_theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="functorch.vjp" href="functorch.vjp.html" />
    <link rel="prev" title="functorch.jacrev" href="functorch.jacrev.html" />


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">

              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../functorch.html">functorch API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../functorch.html#function-transforms">Function Transforms</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="functorch.grad.html">functorch.grad</a></li>
<li class="toctree-l3"><a class="reference internal" href="functorch.grad_and_value.html">functorch.grad_and_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="functorch.jacrev.html">functorch.jacrev</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">functorch.vmap</a></li>
<li class="toctree-l3"><a class="reference internal" href="functorch.vjp.html">functorch.vjp</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../functorch.html#utilities-for-working-with-torch-nn-modules">Utilities for working with torch.nn.Modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="functorch.make_functional.html">functorch.make_functional</a></li>
<li class="toctree-l3"><a class="reference internal" href="functorch.make_functional_with_buffers.html">functorch.make_functional_with_buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="functorch.combine_state_for_ensemble.html">functorch.combine_state_for_ensemble</a></li>
</ul>
</li>
</ul>
</li>
</ul>


        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">functorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../functorch.html">functorch API Reference</a> &raquo;</li>
      <li>functorch.vmap</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/generated/functorch.vmap.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="functorch-vmap">
<h1>functorch.vmap<a class="headerlink" href="#functorch-vmap" title="Permalink to this headline">¶</a></h1>
<dl class="py function">
<dt id="functorch.vmap">
<code class="sig-prename descclassname"><span class="pre">functorch.</span></code><code class="sig-name descname"><span class="pre">vmap</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/functorch/_src/vmap.html#vmap"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#functorch.vmap" title="Permalink to this definition">¶</a></dt>
<dd><p>vmap is the vectorizing map; <code class="docutils literal notranslate"><span class="pre">vmap(func)</span></code> returns a new function that
maps <code class="xref py py-attr docutils literal notranslate"><span class="pre">func</span></code> over some dimension of the inputs. Semantically, vmap
pushes the map into PyTorch operations called by <code class="xref py py-attr docutils literal notranslate"><span class="pre">func</span></code>, effectively
vectorizing those operations.</p>
<p>vmap is useful for handling batch dimensions: one can write a function
<code class="xref py py-attr docutils literal notranslate"><span class="pre">func</span></code> that runs on examples and then lift it to a function that can
take batches of examples with <code class="docutils literal notranslate"><span class="pre">vmap(func)</span></code>. vmap can also be used to
compute batched gradients when composed with autograd.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>function</em>) – A Python function that takes one or more arguments.
Must return one or more Tensors.</p></li>
<li><p><strong>in_dims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> or </em><em>nested structure</em>) – Specifies which dimension of the
inputs should be mapped over. <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_dims</span></code> should have a
structure like the inputs. If the <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_dim</span></code> for a particular
input is None, then that indicates there is no map dimension.
Default: 0.</p></li>
<li><p><strong>out_dims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a><em>]</em>) – Specifies where the mapped dimension
should appear in the outputs. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_dims</span></code> is a Tuple, then
it should have one element per output. Default: 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a new “batched” function. It takes the same inputs as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">func</span></code>, except each input has an extra dimension at the index
specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_dims</span></code>. It takes returns the same outputs as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">func</span></code>, except each output has an extra dimension at the index
specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_dims</span></code>.</p>
</dd>
</dl>
<p>One example of using <a class="reference internal" href="#functorch.vmap" title="functorch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">vmap()</span></code></a> is to compute batched dot products. PyTorch
doesn’t provide a batched <code class="docutils literal notranslate"><span class="pre">torch.dot</span></code> API; instead of unsuccessfully
rummaging through docs, use <a class="reference internal" href="#functorch.vmap" title="functorch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">vmap()</span></code></a> to construct a new function.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span>                            <span class="c1"># [D], [D] -&gt; []</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">)</span>  <span class="c1"># [N, D], [N, D] -&gt; [N]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#functorch.vmap" title="functorch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">vmap()</span></code></a> can be helpful in hiding batch dimensions, leading to a simpler
model authoring experience.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span><span class="p">,</span> <span class="n">feature_size</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">feature_vec</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Very simple linear model with activation</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">feature_vec</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">examples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">model</span><span class="p">)(</span><span class="n">examples</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#functorch.vmap" title="functorch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">vmap()</span></code></a> can also help vectorize computations that were previously difficult
or impossible to batch. One example is higher-order gradient computation.
The PyTorch autograd engine computes vjps (vector-Jacobian products).
Computing a full Jacobian matrix for some function f: R^N -&gt; R^N usually
requires N calls to <code class="docutils literal notranslate"><span class="pre">autograd.grad</span></code>, one per Jacobian row. Using <a class="reference internal" href="#functorch.vmap" title="functorch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">vmap()</span></code></a>,
we can vectorize the whole computation, computing the Jacobian in a single
call to <code class="docutils literal notranslate"><span class="pre">autograd.grad</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Setup</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">I_N</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Sequential approach</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jacobian_rows</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>                 <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">I_N</span><span class="o">.</span><span class="n">unbind</span><span class="p">()]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jacobian</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">jacobian_rows</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># vectorized gradient computation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">get_vjp</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jacobian</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">get_vjp</span><span class="p">)(</span><span class="n">I_N</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#functorch.vmap" title="functorch.vmap"><code class="xref py py-func docutils literal notranslate"><span class="pre">vmap()</span></code></a> can also be nested, producing an output with multiple batched dimensions</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span>                            <span class="c1"># [D], [D] -&gt; []</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">))</span>  <span class="c1"># [N1, N0, D], [N1, N0, D] -&gt; [N1, N0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># tensor of size [2, 3]</span>
</pre></div>
</div>
<p>If the inputs are not batched along the first dimension, <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_dims</span></code> specifies
the dimension that each inputs are batched along as</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span>                            <span class="c1"># [N], [N] -&gt; []</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">,</span> <span class="n">in_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [N, D], [N, D] -&gt; [D]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>   <span class="c1"># output is [5] instead of [2] if batched along the 0th dimension</span>
</pre></div>
</div>
<p>If there are multiple inputs each of which is batched along different dimensions,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_dims</span></code> must be a tuple with the batch dimension for each input as</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span>                            <span class="c1"># [D], [D] -&gt; []</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">,</span> <span class="n">in_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>  <span class="c1"># [N, D], [D] -&gt; [N]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># second arg doesn&#39;t have a batch dim because in_dim[1] was None</span>
</pre></div>
</div>
<p>If the input is a Python struct, <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_dims</span></code> must be a tuple containing a struct
matching the shape of the input:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="nb">dict</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">dict</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_dims</span><span class="o">=</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_dot</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>By default, the output is batched along the first dimension. However, it can be batched
along any dimension by using <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_dims</span></code></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_pow</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">out_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_pow</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># [5, 2]</span>
</pre></div>
</div>
<p>For any function that uses kwargs, the returned function will not batch the kwargs but will
accept kwargs</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">4.</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>  <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">scale</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_pow</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">batched_pow</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batched_pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">x</span><span class="p">)</span> <span class="c1"># scale is not batched, output has shape [2, 2, 5]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>vmap does not provide general autobatching or handle variable-length
sequences out of the box.</p>
</div>
</dd></dl>

</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="functorch.jacrev.html" class="btn btn-neutral float-left" title="functorch.jacrev" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="functorch.vjp.html" class="btn btn-neutral float-right" title="functorch.vjp" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright functorch Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


</body>
</html>