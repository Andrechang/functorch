============================= test session starts ==============================
platform linux -- Python 3.9.4, pytest-6.2.3, py-1.10.0, pluggy-0.13.1 -- /scratch/rzou/pt/debug-cpu-env/bin/python
cachedir: .pytest_cache
rootdir: /private/home/rzou/functorch
collecting ... collected 498 items / 454 deselected / 44 selected
run-last-failure: rerun previous 44 failures

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_diag_embed_cpu_float32 FAILED [  2%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_fftn_cpu_float32 FAILED [  4%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_hfft_cpu_float32 FAILED [  6%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_ifftn_cpu_float32 FAILED [  9%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_irfft_cpu_float32 FAILED [ 11%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_irfftn_cpu_float32 FAILED [ 13%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_float_power_cpu_float32 FAILED [ 15%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_add_cpu_float32 FAILED [ 18%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_copy_cpu_float32 FAILED [ 20%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_fill_cpu_float32 FAILED [ 22%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_put_cpu_float32 FAILED [ 25%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_cholesky_cpu_float32 FAILED [ 27%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_eigvals_cpu_float32 FAILED [ 29%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_eigvalsh_cpu_float32 FAILED [ 31%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_inv_cpu_float32 FAILED [ 34%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_norm_cpu_float32 FAILED [ 36%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_power_cpu_float32 FAILED [ 38%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_rank_cpu_float32 FAILED [ 40%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_rank_hermitian_cpu_float32 FAILED [ 43%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_norm_cpu_float32 FAILED [ 45%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_log_softmax_dtype_cpu_float32 FAILED [ 47%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_logical_not_cpu_float32 FAILED [ 50%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_masked_fill_cpu_float32 FAILED [ 52%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_masked_scatter_cpu_float32 FAILED [ 54%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_nanquantile_cpu_float32 FAILED [ 56%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_norm_fro_cpu_float32 FAILED [ 59%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_norm_nuc_cpu_float32 FAILED [ 61%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_put_cpu_float32 FAILED [ 63%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_quantile_cpu_float32 FAILED [ 65%]
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_signbit_cpu_float32 FAILED [ 68%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_diagonal_cpu FAILED  [ 70%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_index_cpu FAILED     [ 72%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_inplace_manyview_cpu FAILED [ 75%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_inplace_view_cpu FAILED [ 77%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_max_cpu FAILED       [ 79%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_median_cpu FAILED    [ 81%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_min_cpu FAILED       [ 84%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_select_cpu FAILED    [ 86%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_slice_cpu FAILED     [ 88%]
test/test_vmap.py::TestVmapBatchedGradientCPU::test_trace_cpu FAILED     [ 90%]
test/test_vmap.py::TestVmapAPI::test_unsupported_op_err_msg FAILED       [ 93%]
test/test_vmap.py::TestVmapOperators::test_mode_key FAILED               [ 95%]
test/test_vmap.py::TestVmapOperators::test_real FAILED                   [ 97%]
test/test_vmap.py::TestVmapOperators::test_to FAILED                     [100%]

=================================== FAILURES ===================================
____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_diag_embed_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_diag_embed_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c6292be0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676,
          -0.5735,  3.1285, -3.0337],
      ...03],
         [ 0.4932,  2.3851, -7.3627, -4.8190,  4.0836, -6.8625, -1.8878,
           3.9578,  4.6712,  0.5596]]]),)
kwargs = {}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676,
          -0.5735,  3.1285, -3.0337],
      ...903],
         [ 0.4932,  2.3851, -7.3627, -4.8190,  4.0836, -6.8625, -1.8878,
           3.9578,  4.6712,  0.5596]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5ac8b30>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
_____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_fft_fftn_cpu_float32 _____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_fft_fftn_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.SpectralFuncInfo object at 0x7f83c6292e50>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...6,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]]),)
kwargs = {'dim': (1, 2), 'norm': 'ortho', 's': (3, 10)}, batch_size = 3
flat_in_dims = [0]
flat_args = [tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...46,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c53f5450>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
_____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_fft_hfft_cpu_float32 _____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_fft_hfft_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.SpectralFuncInfo object at 0x7f83c6292e80>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...6,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]]),)
kwargs = {'dim': 1, 'n': 10, 'norm': 'ortho'}, batch_size = 3
flat_in_dims = [0]
flat_args = [tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...46,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5ac8a90>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_fft_ifftn_cpu_float32 _____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_fft_ifftn_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.SpectralFuncInfo object at 0x7f83c62a5040>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...6,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]]),)
kwargs = {'dim': (1, 2), 'norm': 'ortho', 's': (3, 10)}, batch_size = 3
flat_in_dims = [0]
flat_args = [tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...46,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55caa90>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_fft_irfft_cpu_float32 _____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_fft_irfft_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.SpectralFuncInfo object at 0x7f83c62a50a0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...6,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]]),)
kwargs = {'dim': 1, 'n': 10, 'norm': 'ortho'}, batch_size = 3
flat_in_dims = [0]
flat_args = [tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...46,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55c2720>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_fft_irfftn_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_fft_irfftn_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.SpectralFuncInfo object at 0x7f83c62a5130>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...6,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]]),)
kwargs = {'dim': (1, 2), 'norm': 'ortho', 's': (3, 10)}, batch_size = 3
flat_in_dims = [0]
flat_args = [tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676],
          [-0.5735,  3.1285, -3.0337,  5.1...46,  7.0563, -2.1266,  6.4973, -4.0044],
          [ 7.4175,  8.9640,  6.1694,  5.1619,  3.3498, -4.4253,  0.1074]]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5ac8450>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
___ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_float_power_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_float_power_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c624bd60>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[0.1459, 2.0105],
         [1.3002, 1.8342]],

        [[0.1459, 2.0105],
         [1.3002, 1.8342]],

     ...81]],

        [[0.1583, 0.8006],
         [0.1518, 0.5681]],

        [[0.1583, 0.8006],
         [0.1518, 0.5681]]]))
kwargs = {}, batch_size = 3, flat_in_dims = [0, 0]
flat_args = [tensor([[[0.1459, 2.0105],
         [1.3002, 1.8342]],

        [[0.1459, 2.0105],
         [1.3002, 1.8342]],

     ...81]],

        [[0.1583, 0.8006],
         [0.1518, 0.5681]],

        [[0.1583, 0.8006],
         [0.1518, 0.5681]]])]
args_spec = TreeSpec(tuple, None, [*, *]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/Batchin...c/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55c2900>)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_index_add_cpu_float32 _____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_index_add_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c61ff7c0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...44],
         [-1.6054, -8.9746,  0.7456,  2.5540, -3.6433],
         [ 3.7379, -1.4590, -7.8209,  6.9096,  5.5491]]]))
kwargs = {}, batch_size = 3, flat_in_dims = [None, None, 0, 0]
flat_args = [tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...44],
         [-1.6054, -8.9746,  0.7456,  2.5540, -3.6433],
         [ 3.7379, -1.4590, -7.8209,  6.9096,  5.5491]]])]
args_spec = TreeSpec(tuple, None, [*, *, *, *]), vmap_level = 2
batched_inputs = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...c/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5482270>)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::index_add_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::index_add_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_index_copy_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_index_copy_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c61ff730>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...44],
         [-1.6054, -8.9746,  0.7456,  2.5540, -3.6433],
         [ 3.7379, -1.4590, -7.8209,  6.9096,  5.5491]]]))
kwargs = {}, batch_size = 3, flat_in_dims = [None, None, 0, 0]
flat_args = [tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...44],
         [-1.6054, -8.9746,  0.7456,  2.5540, -3.6433],
         [ 3.7379, -1.4590, -7.8209,  6.9096,  5.5491]]])]
args_spec = TreeSpec(tuple, None, [*, *, *, *]), vmap_level = 2
batched_inputs = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...c/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55ca040>)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::index_copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::index_copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_index_fill_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_index_fill_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c61ff700>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
         [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
     ...    [-8.5636,  2.9337,  8.6162, -7.1341, -1.9438]]]), 0, tensor([[4],
        [4],
        [4]]), tensor([-1, -1, -1]))
kwargs = {}, batch_size = 3, flat_in_dims = [None, None, 0, 0]
flat_args = [tensor([[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
         [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
     ...    [-8.5636,  2.9337,  8.6162, -7.1341, -1.9438]]]), 0, tensor([[4],
        [4],
        [4]]), tensor([-1, -1, -1])]
args_spec = TreeSpec(tuple, None, [*, *, *, *]), vmap_level = 2
batched_inputs = (tensor([[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
         [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
     ...c/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c53e1860>)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::index_fill_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::index_fill_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_index_put_cpu_float32 _____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_index_put_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c61ff910>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...]],

        [[-6.2551,  2.9920, -2.9822,  5.2072, -3.2105],
         [ 0.4450,  3.0391,  6.1851, -1.3229,  8.2106]]]))
kwargs = {'accumulate': False}, batch_size = 3, flat_in_dims = [None, None, 0]
flat_args = [tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...]],

        [[-6.2551,  2.9920, -2.9822,  5.2072, -3.2105],
         [ 0.4450,  3.0391,  6.1851, -1.3229,  8.2106]]])]
args_spec = TreeSpec(tuple, None, [*, TreeSpec(tuple, None, [*]), *])
vmap_level = 2
batched_inputs = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...c/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55c2c70>)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::index_put_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::index_put_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
_ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_linalg_cholesky_cpu_float32 __

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_linalg_cholesky_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c62b4070>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[ 2.0660,  0.0300,  1.0858,  0.9902, -0.3496],
         [ 0.0300,  2.2251, -2.0522,  0.6769, -1.5608],
     ...3],
         [ 0.9902,  0.6769,  0.0735,  7.0690, -3.1439],
         [-0.3496, -1.5608,  2.1653, -3.1439,  4.4066]]]),)
kwargs = {}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[[ 2.0660,  0.0300,  1.0858,  0.9902, -0.3496],
         [ 0.0300,  2.2251, -2.0522,  0.6769, -1.5608],
     ...53],
         [ 0.9902,  0.6769,  0.0735,  7.0690, -3.1439],
         [-0.3496, -1.5608,  2.1653, -3.1439,  4.4066]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c54a9630>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: Batching rule not implemented for aten::_local_scalar_dense. We could not generate a fallback.

functorch/_src/vmap.py:276: RuntimeError
__ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_linalg_eigvals_cpu_float32 __

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_linalg_eigvals_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c62b41c0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...7],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]]),)
kwargs = {}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...37],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55bf540>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
_ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_linalg_eigvalsh_cpu_float32 __

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_linalg_eigvalsh_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c62b4220>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...7],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]]),)
kwargs = {'UPLO': 'U'}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...37],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83bbb7b810>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_linalg_inv_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_linalg_inv_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c62743a0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...7],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]]),)
kwargs = {}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...37],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5ab6630>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: Batching rule not implemented for aten::_local_scalar_dense. We could not generate a fallback.

functorch/_src/vmap.py:276: RuntimeError
_ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_linalg_matrix_norm_cpu_float32 _

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_linalg_matrix_norm_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c62b46a0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[-8.4784, -1.7658],
         [-4.3228, -2.4005]],

        [[-8.4784, -1.7658],
         [-4.3228, -2.4005]],

        [[-8.4784, -1.7658],
         [-4.3228, -2.4005]]]), 'fro', (-2, -1), True)
kwargs = {}, batch_size = 3, flat_in_dims = [0, None, None, None, None]
flat_args = [tensor([[[-8.4784, -1.7658],
         [-4.3228, -2.4005]],

        [[-8.4784, -1.7658],
         [-4.3228, -2.4005]],

        [[-8.4784, -1.7658],
         [-4.3228, -2.4005]]]), 'fro', -2, -1, True]
args_spec = TreeSpec(tuple, None, [*, *, TreeSpec(tuple, None, [*, *]), *])
vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/Batchin....cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5a9bcc0>, 'fro', (-2, -1), True)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
_ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_linalg_matrix_power_cpu_float32 _

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_linalg_matrix_power_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c62b43a0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[0.5000]],

        [[0.5000]],

        [[0.5000]]]), -4)
kwargs = {}, batch_size = 3, flat_in_dims = [0, None]
flat_args = [tensor([[[0.5000]],

        [[0.5000]],

        [[0.5000]]]), -4]
args_spec = TreeSpec(tuple, None, [*, *]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55bf540>, -4)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: Batching rule not implemented for aten::_local_scalar_dense. We could not generate a fallback.

functorch/_src/vmap.py:276: RuntimeError
_ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_linalg_matrix_rank_cpu_float32 _

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_linalg_matrix_rank_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c62744f0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...7],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]]),)
kwargs = {}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...37],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83bbb7b4f0>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: Batching rule not implemented for aten::sum.IntList_out; the fallback path doesn't work on out= or view ops.

functorch/_src/vmap.py:276: RuntimeError
_ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_linalg_matrix_rank_hermitian_cpu_float32 _

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_linalg_matrix_rank_hermitian_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c6274520>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...7],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]]),)
kwargs = {'hermitian': True}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[[-0.2715,  0.0214, -0.0237, -0.4711, -0.2448],
         [ 0.2292, -0.4486, -0.4759,  0.0295,  0.2052],
     ...37],
         [-0.1122, -0.2536,  0.1319,  0.1296, -0.1279],
         [-0.3454, -0.2721, -0.1876,  0.1717, -0.1563]]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c53e1040>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
___ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_linalg_norm_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_linalg_norm_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c62b45e0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [-8.4784, -1.7658, -4.3228, -2.4005, -7.9506]]),)
kwargs = {'keepdim': False}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [-8.4784, -1.7658, -4.3228, -2.4005, -7.9506]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c59f66d0>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
_ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_log_softmax_dtype_cpu_float32 _

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_log_softmax_dtype_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c621db80>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
          [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
   ...09, -0.8415, -2.7025,  4.3707, -0.7182],
          [-8.5636,  2.9337,  8.6162, -7.1341, -1.9438]]]]), 1, torch.float64)
kwargs = {}, batch_size = 3, flat_in_dims = [0, None, None]
flat_args = [tensor([[[[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
          [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
   ...09, -0.8415, -2.7025,  4.3707, -0.7182],
          [-8.5636,  2.9337,  8.6162, -7.1341, -1.9438]]]]), 1, torch.float64]
args_spec = TreeSpec(tuple, None, [*, *, *]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/Batchin...tions.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83bbb7d400>, 1, torch.float64)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
___ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_logical_not_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_logical_not_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.UnaryUfuncInfo object at 0x7f83c62b4df0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676, -0.5735,
          3.1285, -3.0337,  5.1067, ...3.1285, -3.0337,  5.1067,  1.1351,  4.9473,  5.7744, -3.9730,  3.2708,
         -3.8939,  2.8211, -4.7024,  4.1631]]),)
kwargs = {}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676, -0.5735,
          3.1285, -3.0337,  5.1067, ... 3.1285, -3.0337,  5.1067,  1.1351,  4.9473,  5.7744, -3.9730,  3.2708,
         -3.8939,  2.8211, -4.7024,  4.1631]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5a9bcc0>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: Batching rule not implemented for aten::logical_not.out; the fallback path doesn't work on out= or view ops.

functorch/_src/vmap.py:276: RuntimeError
___ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_masked_fill_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_masked_fill_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c623f190>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...True, False,  True],
         [False, False,  True,  True,  True],
         [False, False,  True,  True,  True]]]), 10)
kwargs = {}, batch_size = 3, flat_in_dims = [None, 0, None]
flat_args = [tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...True, False,  True],
         [False, False,  True,  True,  True],
         [False, False,  True,  True,  True]]]), 10]
args_spec = TreeSpec(tuple, None, [*, *, *]), vmap_level = 2
batched_inputs = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...tchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c59f64a0>, 10)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::masked_fill_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::masked_fill_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
__ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_masked_scatter_cpu_float32 __

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_masked_scatter_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c623f1c0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...91],
         [ 3.7645, -5.8051, -1.0022, -6.7869,  8.3477],
         [ 4.8514, -8.3189, -4.9690,  3.1903,  0.4932]]]))
kwargs = {}, batch_size = 3, flat_in_dims = [None, 0, 0]
flat_args = [tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...91],
         [ 3.7645, -5.8051, -1.0022, -6.7869,  8.3477],
         [ 4.8514, -8.3189, -4.9690,  3.1903,  0.4932]]])]
args_spec = TreeSpec(tuple, None, [*, *, *]), vmap_level = 2
batched_inputs = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506],
        [ 3.6116, -8.0676, -0.5735,  3.1285, -3.0337],
       ...c/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83bbb7bef0>)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::masked_scatter_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::masked_scatter_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
___ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_nanquantile_cpu_float32 ____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_nanquantile_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c624b040>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([-4.3228, -4.3228, -4.3228]), 0.5), kwargs = {}, batch_size = 3
flat_in_dims = [0, None], flat_args = [tensor([-4.3228, -4.3228, -4.3228]), 0.5]
args_spec = TreeSpec(tuple, None, [*, *]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5a9bd60>, 0.5)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: Batching rule not implemented for aten::logical_not.out; the fallback path doesn't work on out= or view ops.

functorch/_src/vmap.py:276: RuntimeError
_____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_norm_fro_cpu_float32 _____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_norm_fro_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c62261f0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <[RuntimeError('batched == nullptrINTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/DynamicLayer.cpp":250, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83bbb7d770>
p = 'fro', dim = None, keepdim = False, out = None, dtype = None

    def norm(input, p="fro", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811
        r"""Returns the matrix norm or vector norm of a given tensor.
    
        .. warning::
    
            torch.norm is deprecated and may be removed in a future PyTorch release.
    
            Use :func:`torch.linalg.norm`, instead, or :func:`torch.linalg.vector_norm`
            when computing vector norms and :func:`torch.linalg.matrix_norm` when
            computing matrix norms. Note, however, the signature for these functions
            is slightly different than the signature for torch.norm.
    
        Args:
            input (Tensor): The input tensor. Its data type must be either a floating
                point or complex type. For complex inputs, the norm is calculated using the
                absolute value of each element. If the input is complex and neither
                :attr:`dtype` nor :attr:`out` is specified, the result's data type will
                be the corresponding floating point type (e.g. float if :attr:`input` is
                complexfloat).
    
            p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``
                The following norms can be calculated:
    
                ======  ==============  ==========================
                ord     matrix norm     vector norm
                ======  ==============  ==========================
                'fro'   Frobenius norm  --
                'nuc'   nuclear norm    --
                Number  --              sum(abs(x)**ord)**(1./ord)
                ======  ==============  ==========================
    
                The vector norm can be calculated across any number of dimensions.
                The corresponding dimensions of :attr:`input` are flattened into
                one dimension, and the norm is calculated on the flattened
                dimension.
    
                Frobenius norm produces the same result as ``p=2`` in all cases
                except when :attr:`dim` is a list of three or more dims, in which
                case Frobenius norm throws an error.
    
                Nuclear norm can only be calculated across exactly two dimensions.
    
            dim (int, tuple of ints, list of ints, optional):
                Specifies which dimension or dimensions of :attr:`input` to
                calculate the norm across. If :attr:`dim` is ``None``, the norm will
                be calculated across all dimensions of :attr:`input`. If the norm
                type indicated by :attr:`p` does not support the specified number of
                dimensions, an error will occur.
            keepdim (bool, optional): whether the output tensors have :attr:`dim`
                retained or not. Ignored if :attr:`dim` = ``None`` and
                :attr:`out` = ``None``. Default: ``False``
            out (Tensor, optional): the output tensor. Ignored if
                :attr:`dim` = ``None`` and :attr:`out` = ``None``.
            dtype (:class:`torch.dtype`, optional): the desired data type of
                returned tensor. If specified, the input tensor is casted to
                :attr:'dtype' while performing the operation. Default: None.
    
        .. note::
            Even though ``p='fro'`` supports any number of dimensions, the true
            mathematical definition of Frobenius norm only applies to tensors with
            exactly two dimensions. :func:`torch.linalg.norm` with ``ord='fro'`` aligns
            with the mathematical definition, since it can only be applied across
            exactly two dimensions.
    
        Example::
    
            >>> import torch
            >>> a = torch.arange(9, dtype= torch.float) - 4
            >>> b = a.reshape((3, 3))
            >>> torch.norm(a)
            tensor(7.7460)
            >>> torch.norm(b)
            tensor(7.7460)
            >>> torch.norm(a, float('inf'))
            tensor(4.)
            >>> torch.norm(b, float('inf'))
            tensor(4.)
            >>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)
            >>> torch.norm(c, dim=0)
            tensor([1.4142, 2.2361, 5.0000])
            >>> torch.norm(c, dim=1)
            tensor([3.7417, 4.2426])
            >>> torch.norm(c, p=1, dim=1)
            tensor([6., 6.])
            >>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)
            >>> torch.norm(d, dim=(1,2))
            tensor([ 3.7417, 11.2250])
            >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
            (tensor(3.7417), tensor(11.2250))
        """
    
        if has_torch_function_unary(input):
            return handle_torch_function(
                norm, (input,), input, p=p, dim=dim, keepdim=keepdim, out=out, dtype=dtype)
    
        ndim = input.dim()
    
        # catch default case
        if dim is None and out is None and dtype is None and p is not None:
            if isinstance(p, str):
                if p == "fro":
>                   return _VF.frobenius_norm(input, dim=(), keepdim=keepdim)
E                   RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/functional.py:1335: RuntimeError
_____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_norm_nuc_cpu_float32 _____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_norm_nuc_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c621deb0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <[RuntimeError('batched == nullptrINTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/DynamicLayer.cpp":250, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c57ef1d0>
p = 'nuc', dim = None, keepdim = False, out = None, dtype = None

    def norm(input, p="fro", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811
        r"""Returns the matrix norm or vector norm of a given tensor.
    
        .. warning::
    
            torch.norm is deprecated and may be removed in a future PyTorch release.
    
            Use :func:`torch.linalg.norm`, instead, or :func:`torch.linalg.vector_norm`
            when computing vector norms and :func:`torch.linalg.matrix_norm` when
            computing matrix norms. Note, however, the signature for these functions
            is slightly different than the signature for torch.norm.
    
        Args:
            input (Tensor): The input tensor. Its data type must be either a floating
                point or complex type. For complex inputs, the norm is calculated using the
                absolute value of each element. If the input is complex and neither
                :attr:`dtype` nor :attr:`out` is specified, the result's data type will
                be the corresponding floating point type (e.g. float if :attr:`input` is
                complexfloat).
    
            p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``
                The following norms can be calculated:
    
                ======  ==============  ==========================
                ord     matrix norm     vector norm
                ======  ==============  ==========================
                'fro'   Frobenius norm  --
                'nuc'   nuclear norm    --
                Number  --              sum(abs(x)**ord)**(1./ord)
                ======  ==============  ==========================
    
                The vector norm can be calculated across any number of dimensions.
                The corresponding dimensions of :attr:`input` are flattened into
                one dimension, and the norm is calculated on the flattened
                dimension.
    
                Frobenius norm produces the same result as ``p=2`` in all cases
                except when :attr:`dim` is a list of three or more dims, in which
                case Frobenius norm throws an error.
    
                Nuclear norm can only be calculated across exactly two dimensions.
    
            dim (int, tuple of ints, list of ints, optional):
                Specifies which dimension or dimensions of :attr:`input` to
                calculate the norm across. If :attr:`dim` is ``None``, the norm will
                be calculated across all dimensions of :attr:`input`. If the norm
                type indicated by :attr:`p` does not support the specified number of
                dimensions, an error will occur.
            keepdim (bool, optional): whether the output tensors have :attr:`dim`
                retained or not. Ignored if :attr:`dim` = ``None`` and
                :attr:`out` = ``None``. Default: ``False``
            out (Tensor, optional): the output tensor. Ignored if
                :attr:`dim` = ``None`` and :attr:`out` = ``None``.
            dtype (:class:`torch.dtype`, optional): the desired data type of
                returned tensor. If specified, the input tensor is casted to
                :attr:'dtype' while performing the operation. Default: None.
    
        .. note::
            Even though ``p='fro'`` supports any number of dimensions, the true
            mathematical definition of Frobenius norm only applies to tensors with
            exactly two dimensions. :func:`torch.linalg.norm` with ``ord='fro'`` aligns
            with the mathematical definition, since it can only be applied across
            exactly two dimensions.
    
        Example::
    
            >>> import torch
            >>> a = torch.arange(9, dtype= torch.float) - 4
            >>> b = a.reshape((3, 3))
            >>> torch.norm(a)
            tensor(7.7460)
            >>> torch.norm(b)
            tensor(7.7460)
            >>> torch.norm(a, float('inf'))
            tensor(4.)
            >>> torch.norm(b, float('inf'))
            tensor(4.)
            >>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)
            >>> torch.norm(c, dim=0)
            tensor([1.4142, 2.2361, 5.0000])
            >>> torch.norm(c, dim=1)
            tensor([3.7417, 4.2426])
            >>> torch.norm(c, p=1, dim=1)
            tensor([6., 6.])
            >>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)
            >>> torch.norm(d, dim=(1,2))
            tensor([ 3.7417, 11.2250])
            >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
            (tensor(3.7417), tensor(11.2250))
        """
    
        if has_torch_function_unary(input):
            return handle_torch_function(
                norm, (input,), input, p=p, dim=dim, keepdim=keepdim, out=out, dtype=dtype)
    
        ndim = input.dim()
    
        # catch default case
        if dim is None and out is None and dtype is None and p is not None:
            if isinstance(p, str):
                if p == "fro":
                    return _VF.frobenius_norm(input, dim=(), keepdim=keepdim)
            if not isinstance(p, str):
                _dim = [i for i in range(ndim)]  # noqa: C416 TODO: rewrite as list(range(m))
                return _VF.norm(input, p, dim=_dim, keepdim=keepdim)  # type: ignore[attr-defined]
    
        # TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed
        # remove the overloads where dim is an int and replace with BraodcastingList1
        # and remove next four lines, replace _dim with dim
        if dim is not None:
            if isinstance(dim, int):
                _dim = [dim]
            else:
                _dim = dim
        else:
            _dim = None  # type: ignore[assignment]
    
        if isinstance(p, str):
            if p == "fro":
                if dtype is not None:
                    raise ValueError("dtype argument is not supported in frobenius norm")
    
                if _dim is None:
                    _dim = list(range(ndim))
                if out is None:
                    return _VF.frobenius_norm(input, _dim, keepdim=keepdim)
                else:
                    return _VF.frobenius_norm(input, _dim, keepdim=keepdim, out=out)
            elif p == "nuc":
                if dtype is not None:
                    raise ValueError("dtype argument is not supported in nuclear norm")
                if _dim is None:
                    if out is None:
>                       return _VF.nuclear_norm(input, keepdim=keepdim)
E                       RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/functional.py:1367: RuntimeError
_______ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_put_cpu_float32 ________

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_put_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c61ffa30>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[ 3.1285, -3.0337,  5.1067],
        [ 1.1351,  4.9473,  5.7744],
        [-3.9730,  3.2708, -3.8939]]), tens... tensor([[-0.8691, -0.4368,  5.1155],
        [-0.8691, -0.4368,  5.1155],
        [-0.8691, -0.4368,  5.1155]]), True)
kwargs = {}, batch_size = 3, flat_in_dims = [None, 0, 0, None]
flat_args = [tensor([[ 3.1285, -3.0337,  5.1067],
        [ 1.1351,  4.9473,  5.7744],
        [-3.9730,  3.2708, -3.8939]]), tens... tensor([[-0.8691, -0.4368,  5.1155],
        [-0.8691, -0.4368,  5.1155],
        [-0.8691, -0.4368,  5.1155]]), True]
args_spec = TreeSpec(tuple, None, [*, *, *, *]), vmap_level = 2
batched_inputs = (tensor([[ 3.1285, -3.0337,  5.1067],
        [ 1.1351,  4.9473,  5.7744],
        [-3.9730,  3.2708, -3.8939]]), <[Ru...hingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c53e14f0>, True)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::put_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::put_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
_____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_quantile_cpu_float32 _____

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_quantile_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.OpInfo object at 0x7f83c623ffa0>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([-4.3228, -4.3228, -4.3228]), 0.5), kwargs = {}, batch_size = 3
flat_in_dims = [0, None], flat_args = [tensor([-4.3228, -4.3228, -4.3228]), 0.5]
args_spec = TreeSpec(tuple, None, [*, *]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55ffa40>, 0.5)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: vmap: aten::masked_fill_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::masked_fill_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

functorch/_src/vmap.py:276: RuntimeError
_____ TestVmapOperatorsOpInfoCPU.test_vmap_exhaustive_signbit_cpu_float32 ______

self = <test_vmap.TestVmapOperatorsOpInfoCPU testMethod=test_vmap_exhaustive_signbit_cpu_float32>
device = 'cpu', dtype = torch.float32
op = <torch.testing._internal.common_methods_invocations.UnaryUfuncInfo object at 0x7f83c6266250>

    @onlyCPU
    @ops(functorch_lagging_op_db, allowed_dtypes=(torch.float,))
    def test_vmap_exhaustive(self, device, dtype, op):
        # These are ops that we can't generate fallbacks for
        op_skip = {
            'broadcast_to',
            'dsplit',
            'hsplit',
            'vsplit',
            'ravel',
            'moveaxis',
            'positive',
            'tensor_split',
            'gradient',
            'fill_',
            'resize_as_',
            'resolve_conj',
            'resize_',
            'to_sparse',
        }
        # Unsupported input types
        if op.name in op_skip:
            return
    
        # entries in here need don't work and need to be fixed.
        # Each one of these is a bug
        vmap_fail = {'__getitem__', 'squeeze', 'unfold'}
        if op.name in vmap_fail:
            return
        sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)
        for sample_input in sample_inputs_itr:
            arg_values = [sample_input.input] + list(sample_input.args)
            kwarg_values = sample_input.kwargs
            # print(arg_values, kwarg_values)
>           for loop_out, batched_out in get_fallback_and_vmap_exhaustive(op.op, arg_values, kwarg_values):

test/test_vmap.py:2860: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/common_utils.py:176: in get_fallback_and_vmap_exhaustive
    batched_out = vmap(op, in_dims=in_dims, out_dims=out_dim)(*batched_args, **kwarg_values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676, -0.5735,
          3.1285, -3.0337,  5.1067, ...3.1285, -3.0337,  5.1067,  1.1351,  4.9473,  5.7744, -3.9730,  3.2708,
         -3.8939,  2.8211, -4.7024,  4.1631]]),)
kwargs = {}, batch_size = 3, flat_in_dims = [0]
flat_args = [tensor([[-8.4784, -1.7658, -4.3228, -2.4005, -7.9506,  3.6116, -8.0676, -0.5735,
          3.1285, -3.0337,  5.1067, ... 3.1285, -3.0337,  5.1067,  1.1351,  4.9473,  5.7744, -3.9730,  3.2708,
         -3.8939,  2.8211, -4.7024,  4.1631]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c59f69a0>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: Batching rule not implemented for aten::signbit.out; the fallback path doesn't work on out= or view ops.

functorch/_src/vmap.py:276: RuntimeError
_________________ TestVmapBatchedGradientCPU.test_diagonal_cpu _________________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_diagonal_cpu>
device = 'cpu'

    def test_diagonal(self, device):
        x = torch.randn(4, 5, device=device, requires_grad=True)
>       self._batched_grad_test(lambda x: x.diagonal(1, 0, 1), (x,))

test/test_vmap.py:2791: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([-0.4966, -0.7120,  0.6970, -0.9919], grad_fn=<DiagonalBackward>),)
inputs = (tensor([[-0.1117, -0.4966,  0.1631, -0.8817,  0.2891],
        [ 0.4899, -0.3853, -0.7120,  0.6369, -0.7141],
       ...831, -0.5547, -1.3248,  0.6970, -0.6631],
        [ 1.2158, -2.5273,  1.4778, -0.1696, -0.9919]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55c2040>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
__________________ TestVmapBatchedGradientCPU.test_index_cpu ___________________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_index_cpu>
device = 'cpu'

    @allowVmapFallbackUsage
    def test_index(self, device):
        x = torch.randn(2, 3, requires_grad=True, device=device)
        index = torch.tensor([[0, 0], [1, 1]], device=device)
    
        def op(x):
            y = x * x
            return y[index]
    
>       self._batched_grad_test(op, (x,))

test/test_vmap.py:2653: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([[[0.0021, 0.1619, 1.0232],
         [0.0021, 0.1619, 1.0232]],

        [[0.0470, 0.3749, 0.2536],
         [0.0470, 0.3749, 0.2536]]], grad_fn=<IndexBackward>),)
inputs = (tensor([[ 0.0461,  0.4024, -1.0115],
        [ 0.2167, -0.6123,  0.5036]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c55ffe00>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::_index_put_impl_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::_index_put_impl_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
_____________ TestVmapBatchedGradientCPU.test_inplace_manyview_cpu _____________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_inplace_manyview_cpu>
device = 'cpu'

    @allowVmapFallbackUsage
    def test_inplace_manyview(self, device):
        leaf = torch.randn(4, 4, 5, requires_grad=True)
    
        def func(leaf):
            # Make sure the function is non-trivially twice differentiable
            base = leaf * leaf
            view = base.transpose(0, 2)
            view = view[1]
            view = view.diagonal()
            view = view[::2]
            view.cos_()
            return view
    
>       self._batched_grad_test(func, (leaf,), {})

test/test_vmap.py:2786: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([ 0.9697, -0.6093], grad_fn=<AsStridedBackward>),)
inputs = (tensor([[[-0.1117, -0.4966,  0.1631, -0.8817,  0.0539],
         [ 0.6684, -0.0597, -0.4675, -0.2153,  0.8840],
     ...0,  0.6144,  0.0628, -0.3297, -1.7970],
         [ 0.8728,  0.7670, -0.1138, -0.9428,  0.7540]]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83bbb7b900>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
_______________ TestVmapBatchedGradientCPU.test_inplace_view_cpu _______________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_inplace_view_cpu>
device = 'cpu'

    @allowVmapFallbackUsage
    def test_inplace_view(self, device):
        leaf = torch.randn(4, 5, requires_grad=True)
    
        def func(leaf):
            # Make sure the function is non-trivially twice differentiable
            base = leaf * leaf
            view = base[0]
            view.cos_()
            return view
    
>       self._batched_grad_test(func, (leaf,), {})

test/test_vmap.py:2769: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([0.9999, 0.9697, 0.9996, 0.7128, 0.9965], grad_fn=<AsStridedBackward>),)
inputs = (tensor([[-0.1117, -0.4966,  0.1631, -0.8817,  0.2891],
        [ 0.4899, -0.3853, -0.7120,  0.6369, -0.7141],
       ...831, -0.5547, -1.3248,  0.6970, -0.6631],
        [ 1.2158, -2.5273,  1.4778, -0.1696, -0.9919]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c53e12c0>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
___________________ TestVmapBatchedGradientCPU.test_max_cpu ____________________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_max_cpu>
device = 'cpu'

    @allowVmapFallbackUsage
    def test_max(self, device):
        x = torch.randn(2, 3, requires_grad=True, device=device)
>       self._batched_grad_test(torch.max, (x,))

test/test_vmap.py:2687: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor(0.5036, grad_fn=<MaxBackward1>),)
inputs = (tensor([[ 0.0461,  0.4024, -1.0115],
        [ 0.2167, -0.6123,  0.5036]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5ab6400>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::masked_fill_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::masked_fill_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
__________________ TestVmapBatchedGradientCPU.test_median_cpu __________________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_median_cpu>
device = 'cpu'

    @allowVmapFallbackUsage
    def test_median(self, device):
        x = torch.randn(2, 3, requires_grad=True, device=device)
>       self._batched_grad_test(torch.median, (x,))

test/test_vmap.py:2692: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor(0.0461, grad_fn=<MedianBackward0>),)
inputs = (tensor([[ 0.0461,  0.4024, -1.0115],
        [ 0.2167, -0.6123,  0.5036]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c59f68b0>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::masked_fill_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::masked_fill_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
___________________ TestVmapBatchedGradientCPU.test_min_cpu ____________________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_min_cpu>
device = 'cpu'

    @allowVmapFallbackUsage
    def test_min(self, device):
        x = torch.randn(2, 3, requires_grad=True, device=device)
>       self._batched_grad_test(torch.min, (x,))

test/test_vmap.py:2697: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor(-1.0115, grad_fn=<MinBackward1>),)
inputs = (tensor([[ 0.0461,  0.4024, -1.0115],
        [ 0.2167, -0.6123,  0.5036]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5609bd0>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::masked_fill_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::masked_fill_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
__________________ TestVmapBatchedGradientCPU.test_select_cpu __________________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_select_cpu>
device = 'cpu'

    def test_select(self, device):
        x = torch.randn(2, 3, device=device, requires_grad=True)
>       self._batched_grad_test(lambda x: x[1], (x,))

test/test_vmap.py:2730: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([ 0.2167, -0.6123,  0.5036], grad_fn=<SelectBackward>),)
inputs = (tensor([[ 0.0461,  0.4024, -1.0115],
        [ 0.2167, -0.6123,  0.5036]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c53e18b0>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
__________________ TestVmapBatchedGradientCPU.test_slice_cpu ___________________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_slice_cpu>
device = 'cpu'

    def test_slice(self, device):
        x = torch.randn(2, 3, 5, device=device, requires_grad=True)
>       self._batched_grad_test(lambda x: x[0:1], (x,))

test/test_vmap.py:2736: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([[[-0.1117, -0.4966,  0.1631, -0.8817,  0.0539],
         [ 0.6684, -0.0597, -0.4675, -0.2153,  0.8840],
         [-0.7584, -0.3689, -0.3424, -1.4020,  1.4255]]],
       grad_fn=<SliceBackward>),)
inputs = (tensor([[[-0.1117, -0.4966,  0.1631, -0.8817,  0.0539],
         [ 0.6684, -0.0597, -0.4675, -0.2153,  0.8840],
     ...5, -0.2772, -0.4030,  0.4195,  0.9380],
         [ 0.0078, -0.3139, -1.1567,  1.8409, -1.0174]]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83bbb7f9f0>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
__________________ TestVmapBatchedGradientCPU.test_trace_cpu ___________________

self = <test_vmap.TestVmapBatchedGradientCPU testMethod=test_trace_cpu>
device = 'cpu'

    def test_trace(self, device):
        x = torch.randn(2, 3, device=device, requires_grad=True)
>       self._batched_grad_test(Tensor.trace, (x,))

test/test_vmap.py:2742: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:2551: in _batched_grad_test
    self._vmap_test(vector_jacobian_product, batched_vectors,
test/test_vmap.py:2533: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
test/test_vmap.py:2549: in vector_jacobian_product
    return torch.autograd.grad(outputs, differentiable(args), vectors,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor(-0.5661, grad_fn=<TraceBackward>),)
inputs = (tensor([[ 0.0461,  0.4024, -1.0115],
        [ 0.2167, -0.6123,  0.5036]], requires_grad=True),)
grad_outputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchingRegistrations.cpp":75, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5a8cbd0>,)
retain_graph = True, create_graph = False, only_inputs = True
allow_unused = False

    def grad(
        outputs: _TensorOrTensors,
        inputs: _TensorOrTensors,
        grad_outputs: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        only_inputs: bool = True,
        allow_unused: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        r"""Computes and returns the sum of gradients of outputs with respect to
        the inputs.
    
        ``grad_outputs`` should be a sequence of length matching ``output``
        containing the "vector" in Jacobian-vector product, usually the pre-computed
        gradients w.r.t. each of the outputs. If an output doesn't require_grad,
        then the gradient can be ``None``).
    
        If ``only_inputs`` is ``True``, the function will only return a list of gradients
        w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
        leaves will still be computed, and will be accumulated into their ``.grad``
        attribute.
    
        .. note::
    
            If you run any forward ops, create ``grad_outputs``, and/or call ``grad``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        Args:
            outputs (sequence of Tensor): outputs of the differentiated function.
            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
                returned (and not accumulated into ``.grad``).
            grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
                Usually gradients w.r.t. each output. None values can be specified for scalar
                Tensors or ones that don't require grad. If a None value would be acceptable
                for all grad_tensors, then this argument is optional. Default: None.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Default: ``False``.
            allow_unused (bool, optional): If ``False``, specifying inputs that were not
                used when computing outputs (and therefore their grad is always zero)
                is an error. Defaults to ``False``.
        """
        outputs = (outputs,) if isinstance(outputs, torch.Tensor) else tuple(outputs)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)
        overridable_args = outputs + inputs
        if has_torch_function(overridable_args):
            return handle_torch_function(
                grad,
                overridable_args,
                outputs,
                inputs,
                grad_outputs=grad_outputs,
                retain_graph=retain_graph,
                create_graph=create_graph,
                only_inputs=only_inputs,
                allow_unused=allow_unused,
            )
    
        if not only_inputs:
            warnings.warn("only_inputs argument is deprecated and is ignored now "
                          "(defaults to True). To accumulate gradient for other "
                          "parts of the graph, please use torch.autograd.backward.")
    
        grad_outputs_ = _tensor_or_tensors_to_tuple(grad_outputs, len(outputs))
        grad_outputs_ = _make_grads(outputs, grad_outputs_)
    
        if retain_graph is None:
            retain_graph = create_graph
    
>       return Variable._execution_engine.run_backward(
            outputs, grad_outputs_, retain_graph, create_graph,
            inputs, allow_unused, accumulate_grad=False)
E       RuntimeError: vmap: aten::index_fill_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::index_fill_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

/raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: RuntimeError
___________________ TestVmapAPI.test_unsupported_op_err_msg ____________________

self = <test_vmap.TestVmapAPI testMethod=test_unsupported_op_err_msg>

    def test_unsupported_op_err_msg(self):
        # Unsupported view op
        tensor = torch.randn(2, 3)
        msg = (
            r"Batching rule not implemented for aten::.+; the "
            r"fallback path doesn't work on out= or view ops"
        )
        with self.assertRaisesRegex(RuntimeError, msg):
>           vmap(torch.ravel)(tensor)
E           AssertionError: RuntimeError not raised

test/test_vmap.py:161: AssertionError
_______________________ TestVmapOperators.test_mode_key ________________________

self = <test_vmap.TestVmapOperators testMethod=test_mode_key>

    def test_mode_key(self):
        def vmap_f(x):
            return x + torch.randn(())
    
        def naive_f(x, shape):
            return x + torch.randn(shape)
    
        torch.manual_seed(0)
>       out1 = vmap(vmap(vmap_f))(torch.ones(2, 3))

test/test_vmap.py:2481: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <[RuntimeError('batched == nullptrINTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/DynamicLayer.cpp":250, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83c5a8cbd0>

    def vmap_f(x):
>       return x + torch.randn(())
E       RuntimeError: vmap: We do not yet support calling random operations inside of vmap. Please perform random operations outside of vmap as a workaround

test/test_vmap.py:2475: RuntimeError
_________________________ TestVmapOperators.test_real __________________________

self = <test_vmap.TestVmapOperators testMethod=test_real>

    def test_real(self):
>       self._test_complex_views(torch.real, dtypes=[torch.cfloat, torch.cdouble])

test/test_vmap.py:1686: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:1683: in _test_complex_views
    run_test(op, dtype)
test/test_vmap.py:1671: in run_test
    test(op, [get([B0, 3])])
test/test_vmap.py:1102: in _vmap_view_test
    self._vmap_test(*args, **kwargs, check_view=True)
test/test_vmap.py:1099: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensor([[-0.0790-0.3511j,  0.1153-0.6234j,  0.0381+0.4726j],
        [-0.0422-0.3306j, -0.1522+0.6251j, -0.5363-0.260...   [ 0.6340-0.2099j,  1.1303-0.0331j, -0.8828-0.6016j],
        [-0.5438-1.1035j, -0.3754+0.1540j,  0.9356+1.2848j]]),)
kwargs = {}, batch_size = 7, flat_in_dims = [0]
flat_args = [tensor([[-0.0790-0.3511j,  0.1153-0.6234j,  0.0381+0.4726j],
        [-0.0422-0.3306j, -0.1522+0.6251j, -0.5363-0.260...    [ 0.6340-0.2099j,  1.1303-0.0331j, -0.8828-0.6016j],
        [-0.5438-1.1035j, -0.3754+0.1540j,  0.9356+1.2848j]])]
args_spec = TreeSpec(tuple, None, [*]), vmap_level = 2
batched_inputs = (<[RuntimeError('maybe_level.has_value()INTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp":258, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83bbb7f590>,)

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        _check_out_dims_is_int_or_int_pytree(out_dims, func)
        batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
        vmap_level = _vmap_increment_nesting(batch_size)
        try:
            batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
>           batched_outputs = func(*batched_inputs, **kwargs)
E           RuntimeError: Batching rule not implemented for aten::_view_as_real_physical; the fallback path doesn't work on out= or view ops.

functorch/_src/vmap.py:276: RuntimeError
__________________________ TestVmapOperators.test_to ___________________________

self = <test_vmap.TestVmapOperators testMethod=test_to>

    def test_to(self):
        test = self._vmap_test
        B0, B1 = 7, 11
    
        test(lambda t: t.to('cpu'), (torch.rand(B0),))
>       test(lambda t: t.to(torch.double), (torch.rand(B0),))

test/test_vmap.py:2312: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test/test_vmap.py:1099: in _vmap_test
    return _vmap_test(self, *args, **kwargs)
test/test_vmap.py:989: in _vmap_test
    result = vmap(op, in_dims, out_dims)(*inputs)
functorch/_src/vmap.py:276: in wrapped
    batched_outputs = func(*batched_inputs, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t = <[RuntimeError('batched == nullptrINTERNAL ASSERT FAILED at "/private/home/rzou/functorch/functorch/csrc/DynamicLayer.cpp":250, please report a bug to PyTorch. ') raised in repr()] Tensor object at 0x7f83b8303b80>

>   test(lambda t: t.to(torch.double), (torch.rand(B0),))
E   RuntimeError: vmap: aten::copy_(self, *extra_args) is not possible because there exists a Tensor `other` in extra_args that has more elements than `self`. This happened due to `other` being vmapped over but `self` not being vmapped over at level 2. Please try to use out-of-place operators instead of aten::copy_. If said operator is being called inside the PyTorch framework, please file a bug report instead.

test/test_vmap.py:2312: RuntimeError
=============================== warnings summary ===============================
test/test_vmap.py: 14 warnings
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::copy_ falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_add_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::index_add_.alpha falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_copy_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::index_copy_ falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_fill_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::index_fill_.int_Tensor falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_put_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::index_put_ falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_cholesky_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::linalg_cholesky_ex falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_inv_cpu_float32
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_power_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::linalg_inv_ex falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_rank_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::_svd_helper falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_rank_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::maximum falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_norm_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::linalg_vector_norm falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_masked_fill_cpu_float32
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_quantile_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::masked_fill_.Scalar falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_masked_scatter_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::masked_scatter_ falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_nanquantile_cpu_float32
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_quantile_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::sort falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_nanquantile_cpu_float32
test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_quantile_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::isnan falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_norm_fro_cpu_float32
  /raid/rzou/pt/debug-cpu/torch/functional.py:1335: UserWarning: Batching rule not implemented for aten::norm.ScalarOpt_dim falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    return _VF.frobenius_norm(input, dim=(), keepdim=keepdim)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_norm_fro_cpu_float32
  /raid/rzou/pt/debug-cpu/torch/functional.py:1335: UserWarning: Batching rule not implemented for aten::copy_ falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    return _VF.frobenius_norm(input, dim=(), keepdim=keepdim)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_norm_nuc_cpu_float32
  /raid/rzou/pt/debug-cpu/torch/functional.py:1367: UserWarning: Batching rule not implemented for aten::_svd_helper falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    return _VF.nuclear_norm(input, keepdim=keepdim)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_norm_nuc_cpu_float32
  /raid/rzou/pt/debug-cpu/torch/functional.py:1367: UserWarning: Batching rule not implemented for aten::copy_ falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    return _VF.nuclear_norm(input, keepdim=keepdim)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_put_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::put_ falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_quantile_cpu_float32
  /private/home/rzou/functorch/functorch/_src/vmap.py:276: UserWarning: Batching rule not implemented for aten::any.dim falling back to slow (for loop and stack) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    batched_outputs = func(*batched_inputs, **kwargs)

test/test_vmap.py::TestVmapBatchedGradientCPU::test_index_cpu
  /raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: UserWarning: Batching rule not implemented for aten::_index_put_impl_ falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    return Variable._execution_engine.run_backward(

test/test_vmap.py::TestVmapBatchedGradientCPU::test_inplace_manyview_cpu
test/test_vmap.py::TestVmapBatchedGradientCPU::test_inplace_view_cpu
  /raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: UserWarning: Batching rule not implemented for aten::copy_ falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    return Variable._execution_engine.run_backward(

test/test_vmap.py::TestVmapBatchedGradientCPU::test_max_cpu
test/test_vmap.py::TestVmapBatchedGradientCPU::test_median_cpu
test/test_vmap.py::TestVmapBatchedGradientCPU::test_min_cpu
  /raid/rzou/pt/debug-cpu/torch/autograd/__init__.py:226: UserWarning: Batching rule not implemented for aten::masked_fill_.Tensor falling back to slow (for loop) implementation (Triggered internally at  /private/home/rzou/functorch/functorch/csrc/BatchedFallback.cpp:95.)
    return Variable._execution_engine.run_backward(

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ============================
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_diag_embed_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_fftn_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_hfft_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_ifftn_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_irfft_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_fft_irfftn_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_float_power_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_add_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_copy_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_fill_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_index_put_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_cholesky_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_eigvals_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_eigvalsh_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_inv_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_norm_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_power_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_rank_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_matrix_rank_hermitian_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_linalg_norm_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_log_softmax_dtype_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_logical_not_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_masked_fill_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_masked_scatter_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_nanquantile_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_norm_fro_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_norm_nuc_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_put_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_quantile_cpu_float32
FAILED test/test_vmap.py::TestVmapOperatorsOpInfoCPU::test_vmap_exhaustive_signbit_cpu_float32
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_diagonal_cpu - Run...
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_index_cpu - Runtim...
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_inplace_manyview_cpu
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_inplace_view_cpu
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_max_cpu - RuntimeE...
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_median_cpu - Runti...
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_min_cpu - RuntimeE...
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_select_cpu - Runti...
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_slice_cpu - Runtim...
FAILED test/test_vmap.py::TestVmapBatchedGradientCPU::test_trace_cpu - Runtim...
FAILED test/test_vmap.py::TestVmapAPI::test_unsupported_op_err_msg - Assertio...
FAILED test/test_vmap.py::TestVmapOperators::test_mode_key - RuntimeError: vm...
FAILED test/test_vmap.py::TestVmapOperators::test_real - RuntimeError: Batchi...
FAILED test/test_vmap.py::TestVmapOperators::test_to - RuntimeError: vmap: at...
=============== 44 failed, 454 deselected, 43 warnings in 8.20s ================
